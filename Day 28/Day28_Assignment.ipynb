{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Create one array of actual values and another array of predicted values. Compare the two sets with the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix is:\n",
      " [[3 2]\n",
      " [1 6]]\n",
      "Accuracy Score is:\n",
      " 0.75\n",
      "\n",
      "\n",
      "Precision score is:\n",
      " 0.75\n",
      "\n",
      "\n",
      "Recall score is:\n",
      " 0.6\n",
      "\n",
      "\n",
      "outcome values from a confusion matrix are: \n",
      " 3 2 1 6\n",
      "Classification report is: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.75      0.60      0.67         5\n",
      "           0       0.75      0.86      0.80         7\n",
      "\n",
      "    accuracy                           0.75        12\n",
      "   macro avg       0.75      0.73      0.73        12\n",
      "weighted avg       0.75      0.75      0.74        12\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# importing confusion matrix and classification report libraries in sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report,accuracy_score,precision_score,f1_score,recall_score\n",
    "\n",
    "# assigining actual and predicted values\n",
    "actual_values=[1,0,1,0,0,0,1,1,0,1,0,0]\n",
    "predicted_values=[1,0,1,0,0,0,0,1,0,0,0,1]\n",
    "\n",
    "#confusion matrix\n",
    "matrix=confusion_matrix(actual_values,predicted_values,labels=[1,0])\n",
    "print('Confusion matrix is:\\n',matrix)\n",
    "\n",
    "\n",
    "#Accuracy score\n",
    "accuracy=accuracy_score(actual_values,predicted_values)\n",
    "print('Accuracy Score is:\\n',accuracy)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "#Precision score\n",
    "precision=precision_score(actual_values,predicted_values)\n",
    "print('Precision score is:\\n',precision)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "#recall score\n",
    "recall=recall_score(actual_values,predicted_values)\n",
    "print('Recall score is:\\n',recall)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "# outcome values in a row\n",
    "tp,fn,fp,tn=confusion_matrix(actual_values,predicted_values,labels=[1,0]).reshape(-1)\n",
    "print('outcome values from a confusion matrix are: \\n',tp,fn,fp,tn)\n",
    "\n",
    "#classification report\n",
    "report=classification_report(actual_values,predicted_values,labels=[1,0])\n",
    "print('Classification report is: \\n',report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: Find out the recall, precision, F1 score and confusion matrix with picture"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "confusion matrix\t\tPredicted\t\n",
    "\t\t        cat \t    dog\n",
    "Actual\t   cat\t 42 \t     8\n",
    "           dog\t 18 \t    32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from this confusion matrix, we came to know the values of tp,fn,fp,tn\n",
    "# here we will take predicting positive as 'cat' and negative as 'dog', so the values will be\n",
    "TP=42\n",
    "TN=32\n",
    "FN=18\n",
    "FP=8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall value (TP/TP+FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall value for the given data is: 0.70\n"
     ]
    }
   ],
   "source": [
    "recall=TP/(TP+FN)\n",
    "print(f\"Recall value for the given data is: {recall:4.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision value (TP/TP+FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision value for the given data is: 0.84\n"
     ]
    }
   ],
   "source": [
    "precision=TP/(TP+FP)\n",
    "print(f\"Precision value for the given data is: {precision:4.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy score (TP+TN)/(TP + TN + FP + FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for the given data is: 0.74\n"
     ]
    }
   ],
   "source": [
    "accuracy=(TP+TN)/(TP + TN + FP + FN)\n",
    "print(f\"Accuracy score for the given data is: {accuracy:4.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1-Score [2*precision*recall/(precision+recall)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score value for the given data is: 0.76\n"
     ]
    }
   ],
   "source": [
    "f1score=2*precision*recall/(precision+recall)\n",
    "print(f\"F1-Score value for the given data is: {f1score:4.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
